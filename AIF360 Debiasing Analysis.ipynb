{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from aif360.datasets import GermanDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms import preprocessing, inprocessing, postprocessing\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fairness_metrics(CM:ClassificationMetric):\n",
    "    print(f\"accuracy = {round(CM.accuracy(), 4)}\")\n",
    "    print(f\"theil index (goal:0) = {round(CM.theil_index(), 4)}\")\n",
    "    print(f\"binary confusion matrix = {CM.binary_confusion_matrix()}\")\n",
    "    print(f\"consistency (goal:1) = {round(CM.consistency()[0], 4)}\")\n",
    "    print(f\"false positive rate difference (negative:privileged bias) = {round(CM.false_positive_rate_difference(), 4)}\")\n",
    "    print(f\"false negative rate difference (negative:privileged bias) = {round(CM.false_negative_rate_difference(), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_fairness_metrics(CM1:ClassificationMetric, CM2:ClassificationMetric, side_by_side:bool=False):\n",
    "    if not side_by_side:\n",
    "        print_fairness_metrics(CM1)\n",
    "        print(\"After Fairness Algos are applied\")\n",
    "        print(\"_\"*10)\n",
    "        print_fairness_metrics(CM2)\n",
    "    else:\n",
    "        print(f\"accuracy = {round(CM1.accuracy(), 4)} => {round(CM2.accuracy(), 4)}\")\n",
    "        print(f\"theil index (goal:0) = {round(CM1.theil_index(), 4)} => {round(CM2.theil_index(), 4)}\")\n",
    "        print(f\"binary confusion matrix = {CM1.binary_confusion_matrix()} => {CM2.binary_confusion_matrix()}\")\n",
    "        print(f\"consistency (goal:1) = {round(CM1.consistency()[0], 4)} => {round(CM2.consistency()[0], 4)}\")\n",
    "        print(f\"false positive rate difference (negative:privileged bias) = \", end=\"\")\n",
    "        print(f\"{round(CM1.false_positive_rate_difference(), 4)} => {round(CM2.false_positive_rate_difference(), 4)}\")\n",
    "        print(f\"false negative rate difference (negative:privileged bias) = \", end=\"\")\n",
    "        print(f\"{round(CM1.false_negative_rate_difference(), 4)} => {round(CM2.false_negative_rate_difference(), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_fairness_metrics_as_df(CM1:ClassificationMetric, CM2:ClassificationMetric,\n",
    "                                   intervention:str) -> pd.DataFrame:\n",
    "    metrics = np.array([[round(CM1.accuracy(), 4), round(CM2.accuracy(), 4)],\n",
    "        [round(CM1.theil_index(), 4), round(CM2.theil_index(), 4)],\n",
    "        [round(CM1.consistency()[0], 4), round(CM2.consistency()[0], 4)],\n",
    "        [round(CM1.false_positive_rate_difference(), 4), round(CM2.false_positive_rate_difference(), 4)],\n",
    "        [round(CM1.false_negative_rate_difference(), 4), round(CM2.false_negative_rate_difference(), 4)]]\n",
    "    )\n",
    "    df = pd.DataFrame(metrics, columns=[\"no intervention\", intervention])\n",
    "    df.index = [\"accuracy\", \"theil index\", \"consistency\", \"false positive rate difference\",\n",
    "                \"false negative rate difference\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-337918e33ec5>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-337918e33ec5>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    if preprocessing_algo not None:\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def analyze__debiasing_algos(dataset, privileged_groups, unprivileged_groups, classifier, \n",
    "                             preprocessing_algo:preprocessing = None, inprocessing_algo:inprocessing = None, \n",
    "                             postprocessing_algo:postprocessing = None):\n",
    "    dataset_train, dataset_test = dataset.split([0.7], shuffle = True)\n",
    "    classifier.fit(dataset_train.features, dataset_train.labels.ravel())\n",
    "    results = classifier.predict(dataset_test.features)\n",
    "    \n",
    "    dataset_test_pred = dataset_test.copy()\n",
    "    dataset_test_pred.labels = np.array([results]).transpose()\n",
    "\n",
    "    CM1 = ClassificationMetric(dataset_test,\n",
    "                              dataset_test_pred,\n",
    "                              unprivileged_groups=unprivileged_groups,\n",
    "                              privileged_groups=privileged_groups)\n",
    "\n",
    "    if preprocessing_algo not None:\n",
    "        dataset_train = preprocessing_algo.fit_transform(dataset_train)\n",
    "    if inprocessing_algo not None:\n",
    "        dataset_train = inprocessing_algo.transform(dataset_train)\n",
    "    if postprocessing_algo not None:\n",
    "        \n",
    "        \n",
    "        \n",
    "    _________________________\n",
    "    RW = preprocessing.Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                                 privileged_groups=privileged_groups)\n",
    "\n",
    "    fair_dataset_train = RW.fit_transform(dataset_train)\n",
    "\n",
    "    fair_RF = RandomForestClassifier(n_estimators=1100)\n",
    "    fair_RF.fit(fair_dataset_train.features, fair_dataset_train.labels.ravel())\n",
    "\n",
    "    results = fair_RF.predict(dataset_test.features)\n",
    "\n",
    "    dataset_test_pred = dataset_test.copy()\n",
    "    dataset_test_pred.labels = np.array([results]).transpose()\n",
    "\n",
    "    CM2 = ClassificationMetric(dataset_test,\n",
    "                              dataset_test_pred,\n",
    "                              unprivileged_groups=unprivileged_groups,\n",
    "                              privileged_groups=privileged_groups)\n",
    "    compare_fairness_metrics(CM1, CM2, side_by_side=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GermanDataset(\n",
    "    protected_attribute_names=['age'],\n",
    "    privileged_classes=[lambda x: x >= 25], #age >= 25 is privileged\n",
    "    features_to_drop=['personal_status', 'sex'] #ignore sex-related stuff\n",
    ")\n",
    "\n",
    "dataset_train, dataset_test = dataset.split([0.7], shuffle = True)\n",
    "\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=1100)\n",
    "RF.fit(dataset_train.features, dataset_train.labels.ravel())\n",
    "\n",
    "results = RF.predict(dataset_test.features)\n",
    "\n",
    "dataset_test_pred = dataset_test.copy()\n",
    "dataset_test_pred.labels = np.array([results]).transpose()\n",
    "\n",
    "CM1 = ClassificationMetric(dataset_test,\n",
    "                          dataset_test_pred,\n",
    "                          unprivileged_groups=unprivileged_groups,\n",
    "                          privileged_groups=privileged_groups)\n",
    "\n",
    "RW = preprocessing.Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "\n",
    "fair_dataset_train = RW.fit_transform(dataset_train)\n",
    "\n",
    "fair_RF = RandomForestClassifier(n_estimators=1100)\n",
    "fair_RF.fit(fair_dataset_train.features, fair_dataset_train.labels.ravel())\n",
    "\n",
    "results = fair_RF.predict(dataset_test.features)\n",
    "\n",
    "dataset_test_pred = dataset_test.copy()\n",
    "dataset_test_pred.labels = np.array([results]).transpose()\n",
    "\n",
    "CM2 = ClassificationMetric(dataset_test,\n",
    "                          dataset_test_pred,\n",
    "                          unprivileged_groups=unprivileged_groups,\n",
    "                          privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Dataset metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no intervention</th>\n",
       "      <th>reweighing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.7300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theil index</th>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.1134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consistency</th>\n",
       "      <td>0.6387</td>\n",
       "      <td>0.6387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false positive rate difference</th>\n",
       "      <td>-0.0238</td>\n",
       "      <td>-0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false negative rate difference</th>\n",
       "      <td>0.2066</td>\n",
       "      <td>0.1465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                no intervention  reweighing\n",
       "accuracy                                 0.7167      0.7300\n",
       "theil index                              0.1280      0.1134\n",
       "consistency                              0.6387      0.6387\n",
       "false positive rate difference          -0.0238     -0.0238\n",
       "false negative rate difference           0.2066      0.1465"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"German Dataset metrics\")\n",
    "compare_fairness_metrics_as_df(CM1, CM2, \"reweighing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
