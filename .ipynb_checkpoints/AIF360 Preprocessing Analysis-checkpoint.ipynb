{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "import aif360\n",
    "from aif360.datasets import AdultDataset, BankDataset, CompasDataset, GermanDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms import preprocessing, inprocessing, postprocessing\n",
    "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "            import load_preproc_data_adult, load_preproc_data_german, load_preproc_data_compas\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions\\\n",
    "            import get_distortion_adult, get_distortion_german, get_distortion_compas\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_metrics(CM:ClassificationMetric):\n",
    "    return np.array([\n",
    "        round(CM.accuracy(), 4),\n",
    "        round(CM.theil_index(), 4),\n",
    "        round(CM.consistency()[0], 4),\n",
    "        round(CM.false_positive_rate_difference(), 4),\n",
    "        round(CM.false_negative_rate_difference(), 4),\n",
    "        round(CM.error_rate_difference(), 4),\n",
    "        round(CM.false_discovery_rate_difference(), 4),\n",
    "        round(CM.false_omission_rate_difference(), 4)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary_dataset_metrics(BLDM:BinaryLabelDatasetMetric):\n",
    "    return np.array([\n",
    "        round(BLDM.statistical_parity_difference(), 4), # negative means privileged bias\n",
    "        round(BLDM.base_rate(privileged=True), 4), # 1 means privileged bias\n",
    "        round(BLDM.base_rate(privileged=False), 4), # 1 means unprivileged bias\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_algo(dataset_train, dataset_test, privileged_groups, unprivileged_groups, classifier, \n",
    "                 preprocessing_algo=None):\n",
    "    if preprocessing_algo is not None:\n",
    "        _dataset_train = preprocessing_algo.fit_transform(dataset_train)\n",
    "    else:\n",
    "        _dataset_train = dataset_train\n",
    "    \n",
    "    classifier.fit(_dataset_train.features, dataset_train.labels.ravel())\n",
    "    results = classifier.predict(dataset_test.features)\n",
    "    \n",
    "    dataset_test_pred = dataset_test.copy()\n",
    "    dataset_test_pred.labels = np.array([results]).transpose()\n",
    "\n",
    "    CM = ClassificationMetric(dataset_test,\n",
    "                              dataset_test_pred,\n",
    "                              unprivileged_groups=unprivileged_groups,\n",
    "                              privileged_groups=privileged_groups)\n",
    "    BLDM = BinaryLabelDatasetMetric(_dataset_train,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "    return np.concatenate((run_classification_metrics(CM), run_binary_dataset_metrics(BLDM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optim_preproc_options_dict(dataset):\n",
    "    if isinstance(dataset, aif360.datasets.german_dataset.GermanDataset):\n",
    "        optim_options = {\n",
    "            \"distortion_fun\": get_distortion_german,\n",
    "            \"epsilon\": 0.05,\n",
    "            \"clist\": [0.99, 1.99, 2.99],\n",
    "            \"dlist\": [.1, 0.05, 0]\n",
    "        }\n",
    "    elif isinstance(dataset, aif360.datasets.adult_dataset.AdultDataset):\n",
    "        optim_options = {\n",
    "            \"distortion_fun\": get_distortion_adult,\n",
    "            \"epsilon\": 0.05,\n",
    "            \"clist\": [0.99, 1.99, 2.99],\n",
    "            \"dlist\": [.1, 0.05, 0]\n",
    "        }\n",
    "    elif isinstance(dataset, aif360.datasets.bank_dataset.BankDataset):\n",
    "        optim_options = {\n",
    "            \"distortion_fun\": get_distortion_bank,\n",
    "            \"epsilon\": 0.05,\n",
    "            \"clist\": [0.99, 1.99, 2.99],\n",
    "            \"dlist\": [.1, 0.05, 0]\n",
    "        }\n",
    "    elif isinstance(dataset, aif360.datasets.compas_dataset.CompasDataset):\n",
    "        optim_options = {\n",
    "            \"distortion_fun\": get_distortion_compas,\n",
    "            \"epsilon\": 0.05,\n",
    "            \"clist\": [0.99, 1.99, 2.99],\n",
    "            \"dlist\": [.1, 0.05, 0]\n",
    "        }\n",
    "    return optim_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preproc_algos_on_dataset(dataset, unprivileged_groups, privileged_groups):\n",
    "    dataset_train, dataset_test = dataset.split([0.7], shuffle = True)\n",
    "\n",
    "    RF = RandomForestClassifier(n_estimators=1100)\n",
    "\n",
    "    RW = preprocessing.Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                                 privileged_groups=privileged_groups)\n",
    "\n",
    "    DIR = preprocessing.DisparateImpactRemover()\n",
    "\n",
    "    if not isinstance(dataset, aif360.datasets.bank_dataset.BankDataset):\n",
    "        optim_options = get_optim_preproc_options_dict(dataset)\n",
    "        OP = OptimPreproc(OptTools, optim_options,\n",
    "                      unprivileged_groups = unprivileged_groups,\n",
    "                      privileged_groups = privileged_groups)\n",
    "\n",
    "    #LFR = preprocessing.LFR(unprivileged_groups, privileged_groups)\n",
    "\n",
    "    metrics = analyze_algo(dataset_train.copy(), dataset_test, privileged_groups, unprivileged_groups, RF)\n",
    "    df = pd.DataFrame(metrics, columns=[\"No Intervention\"])\n",
    "    df.index = [\"accuracy\", \"theil index\", \"consistency\", \"false positive rate difference\",\n",
    "                \"false negative rate difference\", \"error rate difference\",\n",
    "                \"false discovery rate difference\", \"false omission rate difference\",\n",
    "                \"stat parity difference\", \"priv base rate\", \"unpriv base rate\"]\n",
    "    df[\"Reweighing\"] = analyze_algo(dataset_train.copy(), dataset_test, privileged_groups, unprivileged_groups, RF, RW)\n",
    "    df[\"Disparate Impact Remover\"] = analyze_algo(dataset_train.copy(), dataset_test, privileged_groups, unprivileged_groups, RF, DIR)\n",
    "    #if not isinstance(dataset, aif360.datasets.bank_dataset.BankDataset):\n",
    "    #    df[\"Optimized Preprocessing\"] = analyze_algo(dataset_train.copy(), dataset_test, privileged_groups, unprivileged_groups, RF, OP)\n",
    "    #df[\"Learning Fair Representations\"] = analyze_algo(dataset_train.copy(), dataset_test, privileged_groups, unprivileged_groups, RF, LFR)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Algos Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_334c5_\">\n",
       "  <caption>German Dataset</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >No Intervention</th>\n",
       "      <th class=\"col_heading level0 col1\" >Reweighing</th>\n",
       "      <th class=\"col_heading level0 col2\" >Disparate Impact Remover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_334c5_level0_row0\" class=\"row_heading level0 row0\" >accuracy</th>\n",
       "      <td id=\"T_334c5_row0_col0\" class=\"data row0 col0\" >0.756700</td>\n",
       "      <td id=\"T_334c5_row0_col1\" class=\"data row0 col1\" >0.750000</td>\n",
       "      <td id=\"T_334c5_row0_col2\" class=\"data row0 col2\" >0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_334c5_level0_row1\" class=\"row_heading level0 row1\" >theil index</th>\n",
       "      <td id=\"T_334c5_row1_col0\" class=\"data row1 col0\" >0.119500</td>\n",
       "      <td id=\"T_334c5_row1_col1\" class=\"data row1 col1\" >0.117300</td>\n",
       "      <td id=\"T_334c5_row1_col2\" class=\"data row1 col2\" >0.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_334c5_level0_row2\" class=\"row_heading level0 row2\" >consistency</th>\n",
       "      <td id=\"T_334c5_row2_col0\" class=\"data row2 col0\" >0.674000</td>\n",
       "      <td id=\"T_334c5_row2_col1\" class=\"data row2 col1\" >0.674000</td>\n",
       "      <td id=\"T_334c5_row2_col2\" class=\"data row2 col2\" >0.674000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_334c5_level0_row3\" class=\"row_heading level0 row3\" >false positive rate difference</th>\n",
       "      <td id=\"T_334c5_row3_col0\" class=\"data row3 col0\" >-0.138900</td>\n",
       "      <td id=\"T_334c5_row3_col1\" class=\"data row3 col1\" >-0.083300</td>\n",
       "      <td id=\"T_334c5_row3_col2\" class=\"data row3 col2\" >-0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_334c5_level0_row4\" class=\"row_heading level0 row4\" >false negative rate difference</th>\n",
       "      <td id=\"T_334c5_row4_col0\" class=\"data row4 col0\" >0.134500</td>\n",
       "      <td id=\"T_334c5_row4_col1\" class=\"data row4 col1\" >0.139800</td>\n",
       "      <td id=\"T_334c5_row4_col2\" class=\"data row4 col2\" >0.143600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_334c5_level0_row5\" class=\"row_heading level0 row5\" >error rate difference</th>\n",
       "      <td id=\"T_334c5_row5_col0\" class=\"data row5 col0\" >0.065400</td>\n",
       "      <td id=\"T_334c5_row5_col1\" class=\"data row5 col1\" >0.086500</td>\n",
       "      <td id=\"T_334c5_row5_col2\" class=\"data row5 col2\" >0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_334c5_level0_row6\" class=\"row_heading level0 row6\" >false discovery rate difference</th>\n",
       "      <td id=\"T_334c5_row6_col0\" class=\"data row6 col0\" >0.004200</td>\n",
       "      <td id=\"T_334c5_row6_col1\" class=\"data row6 col1\" >0.025200</td>\n",
       "      <td id=\"T_334c5_row6_col2\" class=\"data row6 col2\" >0.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_334c5_level0_row7\" class=\"row_heading level0 row7\" >false omission rate difference</th>\n",
       "      <td id=\"T_334c5_row7_col0\" class=\"data row7 col0\" >0.134100</td>\n",
       "      <td id=\"T_334c5_row7_col1\" class=\"data row7 col1\" >0.177000</td>\n",
       "      <td id=\"T_334c5_row7_col2\" class=\"data row7 col2\" >0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_334c5_level0_row8\" class=\"row_heading level0 row8\" >stat parity difference</th>\n",
       "      <td id=\"T_334c5_row8_col0\" class=\"data row8 col0\" >-0.167000</td>\n",
       "      <td id=\"T_334c5_row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
       "      <td id=\"T_334c5_row8_col2\" class=\"data row8 col2\" >-0.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_334c5_level0_row9\" class=\"row_heading level0 row9\" >priv base rate</th>\n",
       "      <td id=\"T_334c5_row9_col0\" class=\"data row9 col0\" >0.717400</td>\n",
       "      <td id=\"T_334c5_row9_col1\" class=\"data row9 col1\" >0.691400</td>\n",
       "      <td id=\"T_334c5_row9_col2\" class=\"data row9 col2\" >0.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_334c5_level0_row10\" class=\"row_heading level0 row10\" >unpriv base rate</th>\n",
       "      <td id=\"T_334c5_row10_col0\" class=\"data row10 col0\" >0.550500</td>\n",
       "      <td id=\"T_334c5_row10_col1\" class=\"data row10 col1\" >0.691400</td>\n",
       "      <td id=\"T_334c5_row10_col2\" class=\"data row10 col2\" >0.550500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25053f3dfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7cd98_\">\n",
       "  <caption>Adult Dataset</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >No Intervention</th>\n",
       "      <th class=\"col_heading level0 col1\" >Reweighing</th>\n",
       "      <th class=\"col_heading level0 col2\" >Disparate Impact Remover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7cd98_level0_row0\" class=\"row_heading level0 row0\" >accuracy</th>\n",
       "      <td id=\"T_7cd98_row0_col0\" class=\"data row0 col0\" >0.842900</td>\n",
       "      <td id=\"T_7cd98_row0_col1\" class=\"data row0 col1\" >0.844000</td>\n",
       "      <td id=\"T_7cd98_row0_col2\" class=\"data row0 col2\" >0.833100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7cd98_level0_row1\" class=\"row_heading level0 row1\" >theil index</th>\n",
       "      <td id=\"T_7cd98_row1_col0\" class=\"data row1 col0\" >0.119800</td>\n",
       "      <td id=\"T_7cd98_row1_col1\" class=\"data row1 col1\" >0.118900</td>\n",
       "      <td id=\"T_7cd98_row1_col2\" class=\"data row1 col2\" >0.121700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7cd98_level0_row2\" class=\"row_heading level0 row2\" >consistency</th>\n",
       "      <td id=\"T_7cd98_row2_col0\" class=\"data row2 col0\" >0.844500</td>\n",
       "      <td id=\"T_7cd98_row2_col1\" class=\"data row2 col1\" >0.844500</td>\n",
       "      <td id=\"T_7cd98_row2_col2\" class=\"data row2 col2\" >0.844500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7cd98_level0_row3\" class=\"row_heading level0 row3\" >false positive rate difference</th>\n",
       "      <td id=\"T_7cd98_row3_col0\" class=\"data row3 col0\" >-0.094800</td>\n",
       "      <td id=\"T_7cd98_row3_col1\" class=\"data row3 col1\" >-0.092900</td>\n",
       "      <td id=\"T_7cd98_row3_col2\" class=\"data row3 col2\" >-0.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7cd98_level0_row4\" class=\"row_heading level0 row4\" >false negative rate difference</th>\n",
       "      <td id=\"T_7cd98_row4_col0\" class=\"data row4 col0\" >0.085300</td>\n",
       "      <td id=\"T_7cd98_row4_col1\" class=\"data row4 col1\" >0.088100</td>\n",
       "      <td id=\"T_7cd98_row4_col2\" class=\"data row4 col2\" >0.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7cd98_level0_row5\" class=\"row_heading level0 row5\" >error rate difference</th>\n",
       "      <td id=\"T_7cd98_row5_col0\" class=\"data row5 col0\" >-0.122200</td>\n",
       "      <td id=\"T_7cd98_row5_col1\" class=\"data row5 col1\" >-0.119900</td>\n",
       "      <td id=\"T_7cd98_row5_col2\" class=\"data row5 col2\" >-0.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7cd98_level0_row6\" class=\"row_heading level0 row6\" >false discovery rate difference</th>\n",
       "      <td id=\"T_7cd98_row6_col0\" class=\"data row6 col0\" >-0.005100</td>\n",
       "      <td id=\"T_7cd98_row6_col1\" class=\"data row6 col1\" >0.001900</td>\n",
       "      <td id=\"T_7cd98_row6_col2\" class=\"data row6 col2\" >0.012700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7cd98_level0_row7\" class=\"row_heading level0 row7\" >false omission rate difference</th>\n",
       "      <td id=\"T_7cd98_row7_col0\" class=\"data row7 col0\" >-0.103100</td>\n",
       "      <td id=\"T_7cd98_row7_col1\" class=\"data row7 col1\" >-0.101900</td>\n",
       "      <td id=\"T_7cd98_row7_col2\" class=\"data row7 col2\" >-0.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7cd98_level0_row8\" class=\"row_heading level0 row8\" >stat parity difference</th>\n",
       "      <td id=\"T_7cd98_row8_col0\" class=\"data row8 col0\" >-0.197500</td>\n",
       "      <td id=\"T_7cd98_row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
       "      <td id=\"T_7cd98_row8_col2\" class=\"data row8 col2\" >-0.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7cd98_level0_row9\" class=\"row_heading level0 row9\" >priv base rate</th>\n",
       "      <td id=\"T_7cd98_row9_col0\" class=\"data row9 col0\" >0.312000</td>\n",
       "      <td id=\"T_7cd98_row9_col1\" class=\"data row9 col1\" >0.248000</td>\n",
       "      <td id=\"T_7cd98_row9_col2\" class=\"data row9 col2\" >0.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7cd98_level0_row10\" class=\"row_heading level0 row10\" >unpriv base rate</th>\n",
       "      <td id=\"T_7cd98_row10_col0\" class=\"data row10 col0\" >0.114500</td>\n",
       "      <td id=\"T_7cd98_row10_col1\" class=\"data row10 col1\" >0.248000</td>\n",
       "      <td id=\"T_7cd98_row10_col2\" class=\"data row10 col2\" >0.114500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25053edf430>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Bhavya\\\\anaconda3\\\\envs\\\\cascade\\\\lib\\\\site-packages\\\\aif360\\\\datasets\\\\..\\\\data\\\\raw\\\\bank\\\\bank-additional-full.csv'\n",
      "To use this class, please download the following file:\n",
      "\n",
      "\thttps://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
      "\n",
      "unzip it and place the files, as-is, in the folder:\n",
      "\n",
      "\tC:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\aif360\\data\\raw\\bank\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\aif360\\datasets\\bank_dataset.py\", line 35, in __init__\n",
      "    df = pd.read_csv(filepath, sep=';', na_values=na_values)\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\", line 51, in __init__\n",
      "    self._open_handles(src, kwds)\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\", line 222, in _open_handles\n",
      "    self.handles = get_handle(\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\common.py\", line 702, in get_handle\n",
      "    handle = open(\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Bhavya\\\\anaconda3\\\\envs\\\\cascade\\\\lib\\\\site-packages\\\\aif360\\\\datasets\\\\..\\\\data\\\\raw\\\\bank\\\\bank-additional-full.csv'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Bhavya\\AppData\\Local\\Temp/ipykernel_19212/2615017641.py\", line 17, in <module>\n",
      "    dataset = BankDataset(\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\aif360\\datasets\\bank_dataset.py\", line 44, in __init__\n",
      "    sys.exit(1)\n",
      "SystemExit: 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Bhavya\\anaconda3\\envs\\cascade\\lib\\inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\aif360\\datasets\\bank_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, label_name, favorable_classes, protected_attribute_names, privileged_classes, instance_weights_name, categorical_features, features_to_keep, features_to_drop, na_values, custom_preprocessing, metadata)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Bhavya\\\\anaconda3\\\\envs\\\\cascade\\\\lib\\\\site-packages\\\\aif360\\\\datasets\\\\..\\\\data\\\\raw\\\\bank\\\\bank-additional-full.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19212/2615017641.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m dataset = BankDataset(\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprotected_attribute_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\aif360\\datasets\\bank_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, label_name, favorable_classes, protected_attribute_names, privileged_classes, instance_weights_name, categorical_features, features_to_keep, features_to_drop, na_values, custom_preprocessing, metadata)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2055\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[0;32m   2056\u001b[0m                            'the full traceback.\\n']\n\u001b[1;32m-> 2057\u001b[1;33m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[0;32m   2058\u001b[0m                                                                      value))\n\u001b[0;32m   2059\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \"\"\"\n\u001b[1;32m--> 754\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m             out_list = (\n\u001b[1;32m--> 629\u001b[1;33m                 self.structured_traceback(\n\u001b[0m\u001b[0;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0metb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cascade\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "dataset = GermanDataset(\n",
    "    protected_attribute_names=['age'],\n",
    "    privileged_classes=[lambda x: x >= 25], #age >= 25 is privileged\n",
    "    features_to_drop=['personal_status', 'sex'] #ignore sex-related stuff\n",
    ")\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "df = run_preproc_algos_on_dataset(dataset, unprivileged_groups, privileged_groups)\n",
    "display(df.style.set_caption(\"German Dataset\"))\n",
    "\n",
    "dataset = AdultDataset()\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "df = run_preproc_algos_on_dataset(dataset, unprivileged_groups, privileged_groups)\n",
    "display(df.style.set_caption(\"Adult Dataset\"))\n",
    "\n",
    "dataset = BankDataset(\n",
    "    protected_attribute_names=['age'],\n",
    "    privileged_classes=[lambda x: x >= 25], #age >= 25 is privileged\n",
    "    features_to_drop=['day_of_week'] #ignore sex-related stuff\n",
    ")\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "df = run_preproc_algos_on_dataset(dataset, unprivileged_groups, privileged_groups)\n",
    "display(df.style.set_caption(\"Bank Dataset\"))\n",
    "\n",
    "dataset = CompasDataset()\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "df = run_preproc_algos_on_dataset(dataset, unprivileged_groups, privileged_groups)\n",
    "display(df.style.set_caption(\"Compas Dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disregard everything underneath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'aif360.datasets.german_dataset.GermanDataset'>\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No Intervention</th>\n",
       "      <th>LFR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.6933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theil index</th>\n",
       "      <td>0.1228</td>\n",
       "      <td>0.1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consistency</th>\n",
       "      <td>0.6900</td>\n",
       "      <td>0.6927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false positive rate difference</th>\n",
       "      <td>-0.5672</td>\n",
       "      <td>-0.1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false negative rate difference</th>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.0891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>error rate difference</th>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.1775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false discovery rate difference</th>\n",
       "      <td>0.0683</td>\n",
       "      <td>0.2067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false omission rate difference</th>\n",
       "      <td>-0.5485</td>\n",
       "      <td>-0.3248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stat parity difference</th>\n",
       "      <td>-0.1190</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>priv base rate</th>\n",
       "      <td>0.7176</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unpriv base rate</th>\n",
       "      <td>0.5985</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 No Intervention     LFR\n",
       "accuracy                                  0.7033  0.6933\n",
       "theil index                               0.1228  0.1037\n",
       "consistency                               0.6900  0.6927\n",
       "false positive rate difference           -0.5672 -0.1508\n",
       "false negative rate difference            0.1801  0.0891\n",
       "error rate difference                     0.0293  0.1775\n",
       "false discovery rate difference           0.0683  0.2067\n",
       "false omission rate difference           -0.5485 -0.3248\n",
       "stat parity difference                   -0.1190  0.0000\n",
       "priv base rate                            0.7176  1.0000\n",
       "unpriv base rate                          0.5985  1.0000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = GermanDataset(\n",
    "    protected_attribute_names=['age'],\n",
    "    privileged_classes=[lambda x: x >= 25], #age >= 25 is privileged\n",
    "    features_to_drop=['personal_status', 'sex'] #ignore sex-related stuff\n",
    ")\n",
    "dataset = load_preproc_data_german(['age'])\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "print(type(dataset))\n",
    "print(isinstance(dataset, aif360.datasets.german_dataset.GermanDataset))\n",
    "dataset_train, dataset_test = dataset.split([0.7], shuffle = True)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=1100)\n",
    "\n",
    "metrics = analyze_algo(dataset_train.copy(), dataset_test, privileged_groups, unprivileged_groups, RF)\n",
    "df = pd.DataFrame(metrics, columns=[\"No Intervention\"])\n",
    "df.index = [\"accuracy\", \"theil index\", \"consistency\", \"false positive rate difference\",\n",
    "            \"false negative rate difference\", \"error rate difference\",\n",
    "            \"false discovery rate difference\", \"false omission rate difference\",\n",
    "            \"stat parity difference\", \"priv base rate\", \"unpriv base rate\"]\n",
    "\n",
    "scale_orig = StandardScaler()\n",
    "dataset_train.features = scale_orig.fit_transform(dataset_train.features)\n",
    "dataset_test.features = scale_orig.transform(dataset_test.features)\n",
    "TR = preprocessing.LFR(unprivileged_groups=unprivileged_groups,\n",
    "         privileged_groups=privileged_groups,\n",
    "         k=10, Ax=0.1, Ay=1.0, Az=2.0,\n",
    "         verbose=0\n",
    "        )\n",
    "df[\"LFR\"] = analyze_algo(dataset_train, dataset_test, privileged_groups, unprivileged_groups, RF, TR)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 10700 rows removed from BankDataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 641.579346\n",
      "epoch 1; iter: 0; batch classifier loss: 32.879578\n",
      "epoch 2; iter: 0; batch classifier loss: 17.136475\n",
      "epoch 3; iter: 0; batch classifier loss: 12.746425\n",
      "epoch 4; iter: 0; batch classifier loss: 9.400681\n",
      "epoch 5; iter: 0; batch classifier loss: 6.140807\n",
      "epoch 6; iter: 0; batch classifier loss: 2.270022\n",
      "epoch 7; iter: 0; batch classifier loss: 2.144179\n",
      "epoch 8; iter: 0; batch classifier loss: 0.836270\n",
      "epoch 9; iter: 0; batch classifier loss: 0.199547\n",
      "epoch 10; iter: 0; batch classifier loss: 0.470885\n",
      "epoch 11; iter: 0; batch classifier loss: 0.412202\n",
      "epoch 12; iter: 0; batch classifier loss: 0.250243\n",
      "epoch 13; iter: 0; batch classifier loss: 0.263953\n",
      "epoch 14; iter: 0; batch classifier loss: 0.381343\n",
      "epoch 15; iter: 0; batch classifier loss: 0.254617\n",
      "epoch 16; iter: 0; batch classifier loss: 0.229809\n",
      "epoch 17; iter: 0; batch classifier loss: 0.286453\n",
      "epoch 18; iter: 0; batch classifier loss: 0.254371\n",
      "epoch 19; iter: 0; batch classifier loss: 0.186551\n",
      "epoch 20; iter: 0; batch classifier loss: 0.314237\n",
      "epoch 21; iter: 0; batch classifier loss: 0.269565\n",
      "epoch 22; iter: 0; batch classifier loss: 0.177701\n",
      "epoch 23; iter: 0; batch classifier loss: 0.305265\n",
      "epoch 24; iter: 0; batch classifier loss: 0.290146\n",
      "epoch 25; iter: 0; batch classifier loss: 0.281725\n",
      "epoch 26; iter: 0; batch classifier loss: 0.235768\n",
      "epoch 27; iter: 0; batch classifier loss: 0.176383\n",
      "epoch 28; iter: 0; batch classifier loss: 0.342565\n",
      "epoch 29; iter: 0; batch classifier loss: 0.170852\n",
      "epoch 30; iter: 0; batch classifier loss: 0.238003\n",
      "epoch 31; iter: 0; batch classifier loss: 0.301277\n",
      "epoch 32; iter: 0; batch classifier loss: 0.263933\n",
      "epoch 33; iter: 0; batch classifier loss: 0.165310\n",
      "epoch 34; iter: 0; batch classifier loss: 0.312401\n",
      "epoch 35; iter: 0; batch classifier loss: 0.244240\n",
      "epoch 36; iter: 0; batch classifier loss: 0.202886\n",
      "epoch 37; iter: 0; batch classifier loss: 0.250031\n",
      "epoch 38; iter: 0; batch classifier loss: 0.224527\n",
      "epoch 39; iter: 0; batch classifier loss: 0.221191\n",
      "epoch 40; iter: 0; batch classifier loss: 0.211093\n",
      "epoch 41; iter: 0; batch classifier loss: 0.251995\n",
      "epoch 42; iter: 0; batch classifier loss: 0.201542\n",
      "epoch 43; iter: 0; batch classifier loss: 0.232339\n",
      "epoch 44; iter: 0; batch classifier loss: 0.193809\n",
      "epoch 45; iter: 0; batch classifier loss: 0.317659\n",
      "epoch 46; iter: 0; batch classifier loss: 0.261861\n",
      "epoch 47; iter: 0; batch classifier loss: 0.228658\n",
      "epoch 48; iter: 0; batch classifier loss: 0.283354\n",
      "epoch 49; iter: 0; batch classifier loss: 0.221239\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-d5d97aa90246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                            scope_name=scope_name_2, sess=sess2)\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mfair_AD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mdataset_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfair_AD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/aif360/algorithms/transformer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnew_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mnew_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/aif360/algorithms/inprocessing/adversarial_debiasing.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassifier_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0munit_adversary_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madversary_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                     \u001b[0mgrad\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_adversary_grad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_adversary_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                     \u001b[0mgrad\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madversary_loss_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madversary_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/aif360/algorithms/inprocessing/adversarial_debiasing.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 adversary_grads = {var: grad for (grad, var) in adversary_opt.compute_gradients(pred_protected_attributes_loss,\n\u001b[1;32m    175\u001b[0m                                                                                       var_list=classifier_vars)}\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtiny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mclassifier_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                 instructions)\n\u001b[0;32m--> 538\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/linalg_ops.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(tensor, ord, axis, keepdims, name, keep_dims)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'fro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    279\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 281\u001b[0;31m       tensor_util.make_tensor_proto(\n\u001b[0m\u001b[1;32m    282\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m           allow_broadcast=allow_broadcast))\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    443\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# provided if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "dataset = BankDataset(\n",
    "    protected_attribute_names=['age'],\n",
    "    privileged_classes=[lambda x: x >= 25], #age >= 25 is privileged\n",
    "    features_to_drop=['day_of_week'] #ignore sex-related stuff\n",
    ")\n",
    "\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "\n",
    "dataset_train, dataset_test = dataset.split([0.7], shuffle = True)\n",
    "\n",
    "sess1 = tf.compat.v1.Session()\n",
    "with tf.compat.v1.variable_scope(\"var1\", reuse=tf.compat.v1.AUTO_REUSE) as scope_name_1:\n",
    "    AD = inprocessing.AdversarialDebiasing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups,\n",
    "                                           scope_name=scope_name_1, sess=sess1, debias=False)\n",
    "\n",
    "    AD.fit(dataset_train)\n",
    "    dataset_test_pred = AD.predict(dataset_test)\n",
    "\n",
    "    CM = ClassificationMetric(dataset_test,\n",
    "                              dataset_test_pred,\n",
    "                              unprivileged_groups=unprivileged_groups,\n",
    "                              privileged_groups=privileged_groups)\n",
    "    BLDM = BinaryLabelDatasetMetric(dataset_train,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "    AD_metrics = np.concatenate((run_classification_metrics(CM), run_binary_dataset_metrics(BLDM)))\n",
    "\n",
    "sess1.close()\n",
    "\n",
    "metrics = AD_metrics\n",
    "df = pd.DataFrame(metrics, columns=[\"No Intervention\"])\n",
    "df.index = [\"accuracy\", \"theil index\", \"consistency\", \"false positive rate difference\",\n",
    "            \"false negative rate difference\", \"error rate difference\",\n",
    "            \"false discovery rate difference\", \"false omission rate difference\",\n",
    "            \"stat parity difference\", \"priv base rate\", \"unpriv base rate\"]\n",
    "\n",
    "sess2 = tf.compat.v1.Session()\n",
    "with tf.compat.v1.variable_scope(\"var2\", reuse=tf.compat.v1.AUTO_REUSE) as scope_name_2:\n",
    "    fair_AD = inprocessing.AdversarialDebiasing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups,\n",
    "                                           scope_name=scope_name_2, sess=sess2)\n",
    "\n",
    "    fair_AD.fit(dataset_train)\n",
    "    dataset_test_pred = fair_AD.predict(dataset_test)\n",
    "\n",
    "    CM = ClassificationMetric(dataset_test,\n",
    "                              dataset_test_pred,\n",
    "                              unprivileged_groups=unprivileged_groups,\n",
    "                              privileged_groups=privileged_groups)\n",
    "    BLDM = BinaryLabelDatasetMetric(dataset_train,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "    df[\"Adversarial Debiasing w/o dataset\"] = np.concatenate((run_classification_metrics(CM), run_binary_dataset_metrics(BLDM)))\n",
    "sess2.close()\n",
    "\n",
    "'''sess3 = tf.compat.v1.Session()\n",
    "with tf.compat.v1.variable_scope(\"var3\", reuse=tf.compat.v1.AUTO_REUSE) as scope_name_3:\n",
    "    fair_AD = inprocessing.AdversarialDebiasing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups,\n",
    "                                           scope_name=scope_name_3, sess=sess3, debias=True)\n",
    "\n",
    "    fair_AD.fit(dataset_train.copy())\n",
    "    fair_dataset_train = fair_AD.transform(dataset_train)\n",
    "    fair_AD = inprocessing.AdversarialDebiasing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups,\n",
    "                                           scope_name=scope_name_3, sess=sess3, debias=True)\n",
    "    fair_AD.fit(fair_dataset_train)\n",
    "    dataset_test_pred = fair_AD.predict(dataset_test)\n",
    "\n",
    "    CM = ClassificationMetric(dataset_test,\n",
    "                              dataset_test_pred,\n",
    "                              unprivileged_groups=unprivileged_groups,\n",
    "                              privileged_groups=privileged_groups)\n",
    "    BLDM = BinaryLabelDatasetMetric(dataset_train,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "    df[\"Adversarial Debiasing w dataset\"] = np.concatenate((run_classification_metrics(CM), run_binary_dataset_metrics(BLDM)))\n",
    "\n",
    "sess3.close()'''\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 58)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-1ab7fdc713fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mPR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrejudiceRemover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mPR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mPR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/aif360/algorithms/transformer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnew_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mnew_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/aif360/algorithms/inprocessing/prejudice_remover.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mPrejudiceRemover\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \"\"\"\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "dataset = GermanDataset(\n",
    "    protected_attribute_names=['age'],\n",
    "    privileged_classes=[lambda x: x >= 25], #age >= 25 is privileged\n",
    "    features_to_drop=['personal_status', 'sex'] #ignore sex-related stuff\n",
    ")\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "\n",
    "dataset_train, dataset_test = dataset.split([0.7], shuffle = True)\n",
    "\n",
    "df_dataset = dataset_train.convert_to_dataframe()\n",
    "\n",
    "print(df_dataset[0].shape)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=1100)\n",
    "\n",
    "PR = inprocessing.PrejudiceRemover()\n",
    "\n",
    "PR.fit(dataset_train.features)\n",
    "PR.transform(dataset_train)\n",
    "\n",
    "'''metrics = analyze_algo(dataset_train.copy(), dataset_test, privileged_groups, unprivileged_groups, RF)\n",
    "df = pd.DataFrame(metrics, columns=[\"No Intervention\"])\n",
    "df.index = [\"accuracy\", \"theil index\", \"consistency\", \"false positive rate difference\",\n",
    "            \"false negative rate difference\", \"error rate difference\",\n",
    "            \"false discovery rate difference\", \"false omission rate difference\",\n",
    "            \"stat parity difference\", \"priv base rate\", \"unpriv base rate\"]\n",
    "df[\"ART\"] = analyze_algo(dataset_train.copy(), dataset_test, privileged_groups, unprivileged_groups, RF, ART)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fairness_metrics(CM:ClassificationMetric):\n",
    "    print(f\"accuracy = {round(CM.accuracy(), 4)}\")\n",
    "    print(f\"theil index (goal:0) = {round(CM.theil_index(), 4)}\")\n",
    "    print(f\"binary confusion matrix = {CM.binary_confusion_matrix()}\")\n",
    "    print(f\"consistency (goal:1) = {round(CM.consistency()[0], 4)}\")\n",
    "    print(f\"false positive rate difference (negative:privileged bias) = {round(CM.false_positive_rate_difference(), 4)}\")\n",
    "    print(f\"false negative rate difference (negative:privileged bias) = {round(CM.false_negative_rate_difference(), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_fairness_metrics_as_df(CM1:ClassificationMetric, CM2:ClassificationMetric,\n",
    "                                   intervention:str) -> pd.DataFrame:\n",
    "    metrics = np.array([[round(CM1.accuracy(), 4), round(CM2.accuracy(), 4)],\n",
    "        [round(CM1.theil_index(), 4), round(CM2.theil_index(), 4)],\n",
    "        [round(CM1.consistency()[0], 4), round(CM2.consistency()[0], 4)],\n",
    "        [round(CM1.false_positive_rate_difference(), 4), round(CM2.false_positive_rate_difference(), 4)],\n",
    "        [round(CM1.false_negative_rate_difference(), 4), round(CM2.false_negative_rate_difference(), 4)]]\n",
    "    )\n",
    "    df = pd.DataFrame(metrics, columns=[\"no intervention\", intervention])\n",
    "    df.index = [\"accuracy\", \"theil index\", \"consistency\", \"false positive rate difference\",\n",
    "                \"false negative rate difference\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_debiasing_algos(dataset, privileged_groups, unprivileged_groups, classifier, \n",
    "                             preprocessing_algo:preprocessing = None, inprocessing_algo:inprocessing = None, \n",
    "                             postprocessing_algo:postprocessing = None):\n",
    "    dataset_train, dataset_test = dataset.split([0.7], shuffle = True)\n",
    "    classifier.fit(dataset_train.features, dataset_train.labels.ravel())\n",
    "    results = classifier.predict(dataset_test.features)\n",
    "    \n",
    "    dataset_test_pred = dataset_test.copy()\n",
    "    dataset_test_pred.labels = np.array([results]).transpose()\n",
    "\n",
    "    CM1 = ClassificationMetric(dataset_test,\n",
    "                              dataset_test_pred,\n",
    "                              unprivileged_groups=unprivileged_groups,\n",
    "                              privileged_groups=privileged_groups)\n",
    "\n",
    "    if preprocessing_algo is not None:\n",
    "        dataset_train = preprocessing_algo.fit_transform(dataset_train)\n",
    "    fair_classifier = clone(classifier)\n",
    "    fair_classifier.fit(dataset_train.features, dataset_train.labels.ravel())\n",
    "    \n",
    "    results = fair_classifier.predict(dataset_test.features)\n",
    "    \n",
    "    dataset_test_pred = dataset_test.copy()\n",
    "    dataset_test_pred.labels = np.array([results]).transpose()\n",
    "    \n",
    "    CM2 = ClassificationMetric(dataset_test,\n",
    "                              dataset_test_pred,\n",
    "                              unprivileged_groups=unprivileged_groups,\n",
    "                              privileged_groups=privileged_groups)\n",
    "    return compare_fairness_metrics_as_df(CM1, CM2, \"preprocessing\")\n",
    "        \n",
    "    '''_________________________\n",
    "    RW = preprocessing.Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                                 privileged_groups=privileged_groups)\n",
    "\n",
    "    fair_dataset_train = RW.fit_transform(dataset_train)\n",
    "\n",
    "    fair_RF = RandomForestClassifier(n_estimators=1100)\n",
    "    fair_RF.fit(fair_dataset_train.features, fair_dataset_train.labels.ravel())\n",
    "\n",
    "    results = fair_RF.predict(dataset_test.features)\n",
    "\n",
    "    dataset_test_pred = dataset_test.copy()\n",
    "    dataset_test_pred.labels = np.array([results]).transpose()\n",
    "\n",
    "    CM2 = ClassificationMetric(dataset_test,\n",
    "                              dataset_test_pred,\n",
    "                              unprivileged_groups=unprivileged_groups,\n",
    "                              privileged_groups=privileged_groups)\n",
    "    compare_fairness_metrics(CM1, CM2, side_by_side=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GermanDataset(\n",
    "    protected_attribute_names=['age'],\n",
    "    privileged_classes=[lambda x: x >= 25], #age >= 25 is privileged\n",
    "    features_to_drop=['personal_status', 'sex'] #ignore sex-related stuff\n",
    ")\n",
    "\n",
    "dataset_train, dataset_test = dataset.split([0.7], shuffle = True)\n",
    "\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=1100)\n",
    "RF.fit(dataset_train.features, dataset_train.labels.ravel())\n",
    "\n",
    "results = RF.predict(dataset_test.features)\n",
    "\n",
    "dataset_test_pred = dataset_test.copy()\n",
    "dataset_test_pred.labels = np.array([results]).transpose()\n",
    "\n",
    "CM1 = ClassificationMetric(dataset_test,\n",
    "                          dataset_test_pred,\n",
    "                          unprivileged_groups=unprivileged_groups,\n",
    "                          privileged_groups=privileged_groups)\n",
    "\n",
    "RW = preprocessing.LFR(unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "\n",
    "fair_dataset_train = RW.fit_transform(dataset_train)\n",
    "\n",
    "fair_RF = RandomForestClassifier(n_estimators=1100)\n",
    "fair_RF.fit(fair_dataset_train.features, fair_dataset_train.labels.ravel())\n",
    "\n",
    "results = fair_RF.predict(dataset_test.features)\n",
    "\n",
    "dataset_test_pred = dataset_test.copy()\n",
    "dataset_test_pred.labels = np.array([results]).transpose()\n",
    "\n",
    "CM2 = ClassificationMetric(dataset_test,\n",
    "                          dataset_test_pred,\n",
    "                          unprivileged_groups=unprivileged_groups,\n",
    "                          privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Dataset metrics\n",
      "{'TP': 192.0, 'FP': 51.0, 'TN': 40.0, 'FN': 17.0} {'TP': 209.0, 'FP': 91.0, 'TN': 0.0, 'FN': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"German Dataset metrics\")\n",
    "compare_fairness_metrics_as_df(CM1, CM2, \"reweighing\")\n",
    "print(CM1.binary_confusion_matrix(), CM2.binary_confusion_matrix())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
